version: '3.9'

services:
  postgres:
    image: postgres:latest
    container_name: postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - ${POSTGRES_PORT}:${POSTGRES_PORT} # Exposing PostgreSQL port to the host machine
    volumes:
      - ./postgres_data:/var/lib/postgresql/data  # Mount the volume to the container
      - ./api-service/postgresql/sql/up_v1.sql:/docker-entrypoint-initdb.d/up_v1.sql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 30s
      retries: 3
      timeout: 10s
      start_period: 5s
    networks:
      - api

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000" # S3-compatible API
      - "9001:9001" # MinIO Console (web UI)
    environment:
      MINIO_ROOT_USER:  ${MINIO_USER}        # MinIO access key
      MINIO_ROOT_PASSWORD:  ${MINIO_PASSWORD}   # MinIO secret key
    volumes:
      - ./minio_data:/data                   # Persist MinIO data on host
    command: server /data
    networks:
      - api

  api-service:
    build: ./api-service
    ports:
      - "8080:8080"                           # Expose the API port
    depends_on:
      - minio
    environment:
      MINIO_ENDPOINT:  ${MINIO_ENDPOINT}   # Endpoint of MinIO in the compose setup
      MINIO_ACCESS_KEY: ${MINIO_USER}         # MinIO access key
      MINIO_SECRET_KEY: ${MINIO_PASSWORD}     # MinIO secret key
      AIRFLOW_USER: ${AIRFLOW_USER}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      HOST_DB:  ${HOST_DB}
      SCRAPER_BUCKET: ${SCRAPER_BUCKET}
    networks:
      - api

  # # Optional: Your app container here
  # app:
  #   image: your-app-image
  #   container_name: your-app
  #   depends_on:
  #     - db
  #   networks:
  #     - postgres-network
  #   environment:
  #     DATABASE_URL: "postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}"


  # airflow-webserver:
  #   image: apache/airflow:latest
  #   container_name: airflow-webserver
  #   entrypoint: ['/opt/airflow/script/entrypoint.sh']
  #   restart: always
  #   depends_on:
  #     - postgres
  #     - redis
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  #     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/airflow
  #     AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
  #     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/airflow
  #     AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER}
  #     AIRFLOW_WWW_EMAIL: ${AIRFLOW_WWW_EMAIL}
  #     AIRFLOW_WWW_USER: ${AIRFLOW_WWW_USER}
  #     AIRFLOW_WWW_PASSWORD: ${AIRFLOW_WWW_PASSWORD}
  #   volumes:
  #     - ./airflow/entrypoint.sh:/opt/airflow/script/entrypoint.sh
  #     - ./airflow:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  #   ports:
  #     - "8081:8080"  # Airflow UI on port 8081
  #   command: bash -c "airflow webserver && chmod +x /opt/airflow/script/entrypoint.sh"
  #   healthcheck:
  #     test: ['CMD-SHELL', "[ -f /opt/airflow/airflow-webserver.pid ]"]
  #     interval: 30s
  #     timeout: 30s
  #     retries: 3

  # airflow-scheduler:
  #   image: apache/airflow:latest
  #   container_name: airflow-scheduler
  #   restart: always
  #   depends_on:
  #     - airflow-webserver
  #     - redis
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  #     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/airflow
  #     AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
  #     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/airflow
  #     AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER}
  #   command: bash -c "airflow db migrate && airflow scheduler"

  # airflow-worker:
  #   image: apache/airflow:latest
  #   container_name: airflow-worker
  #   restart: always
  #   depends_on:
  #     - airflow-scheduler
  #     - redis
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  #     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/airflow
  #     AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
  #     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/airflow
  #     AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER}
  #     DUMB_INIT_SETSID: "0"
  #   command: celery worker

  webserver:
    image: apache/airflow:latest
    command: bash -c "airflow webserver && chmod +x /opt/airflow/script/entrypoint.sh "
    entrypoint: ['/opt/airflow/script/entrypoint.sh']
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=Sequential
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_WEBSERVER}
      - AIRFLOW_WWW_EMAIL=${AIRFLOW_WWW_EMAIL}
      - AIRFLOW_WWW_USER=${AIRFLOW_WWW_USER}
      - AIRFLOW_WWW_PASSWORD=${AIRFLOW_WWW_PASSWORD}
      - AIRFLOW_USER=${AIRFLOW_USER}
      - AIRFLOW__LOGGING__BASE_LOG_FOLDER=/opt/airflow/logs
      - TZ=Asia/Singapore  
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/script:/opt/airflow/script
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/models:/opt/airflow/models
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt

    ports:
      - "8081:8080"
    healthcheck:
      test: ['CMD-SHELL', "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3
    networks:
      - api


  scheduler:
    image: apache/airflow:latest
    depends_on:
      webserver:
        condition: service_healthy
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/models:/opt/airflow/models
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
    environment:
      - LOAD_EX=n
      - EXECUTOR=Sequential
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_WEBSERVER}
      - AIRFLOW_USER=${AIRFLOW_USER}
      - AIRFLOW__LOGGING__BASE_LOG_FOLDER=/opt/airflow/logs
      - TZ=Asia/Singapore  

    command: bash -c "python3 -m pip install --upgrade pip && pip install --no-cache-dir -r /opt/airflow/requirements.txt && airflow db migrate && airflow scheduler"
    networks:
      - api


volumes:
  postgres_data:
  minio_data:
  
networks:
  api:
    driver: bridge